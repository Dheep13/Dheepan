{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGBackend Logic Summary\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "1. **PDF Processing**\n",
    "   - Extracts text from PDF documents\n",
    "   - Splits text into smaller chunks for efficient processing\n",
    "\n",
    "2. **Vector Store**\n",
    "   - Creates embeddings for text chunks\n",
    "   - Stores these embeddings for quick retrieval\n",
    "\n",
    "3. **Question-Answering Chain**\n",
    "   - Uses a language model (LLM) to generate answers\n",
    "   - Retrieves relevant information from the vector store\n",
    "\n",
    "## Process Flow:\n",
    "\n",
    "1. **Initialization**\n",
    "   - RAGBackend is initialized with an OpenAI API key\n",
    "   - Sets up necessary components (vector store, QA chain)\n",
    "\n",
    "2. **PDF Processing (`process_pdf` method)**\n",
    "   - Extracts text from the uploaded PDF\n",
    "   - Splits the text into manageable chunks\n",
    "   - Creates embeddings for these chunks\n",
    "   - Stores the embeddings in a vector store (FAISS)\n",
    "\n",
    "3. **Question Answering (`ask_question` method)**\n",
    "   - Receives a question from the user\n",
    "   - Uses the vector store to retrieve relevant text chunks\n",
    "   - Passes the question and relevant chunks to the LLM\n",
    "   - Returns the generated answer\n",
    "\n",
    "## Key Methods:\n",
    "\n",
    "1. `_extract_text_from_pdf(pdf_path)`:\n",
    "   - Uses PyPDF2 to extract text from each page of the PDF\n",
    "\n",
    "2. `_split_text(text)`:\n",
    "   - Uses LangChain's RecursiveCharacterTextSplitter to split text into chunks\n",
    "\n",
    "3. `_create_vector_store(chunks)`:\n",
    "   - Uses OpenAIEmbeddings to create embeddings\n",
    "   - Stores these in a FAISS vector store\n",
    "\n",
    "4. `_setup_qa_chain()`:\n",
    "   - Sets up a RetrievalQA chain using the OpenAI language model and the vector store\n",
    "\n",
    "5. `process_pdf(pdf_path)`:\n",
    "   - Orchestrates the PDF processing workflow\n",
    "\n",
    "6. `ask_question(question)`:\n",
    "   - Handles the question-answering process\n",
    "\n",
    "## Key Technologies:\n",
    "\n",
    "- LangChain: For text splitting, embeddings, and QA chain setup\n",
    "- OpenAI: For generating embeddings and language model responses\n",
    "- FAISS: For efficient similarity search in the vector store\n",
    "- PyPDF2: For PDF text extraction\n",
    "\n",
    "This RAG system enhances traditional question-answering by grounding the LLM's responses in the specific content of the processed PDF, improving accuracy and relevance of the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying RAG Application to SAP Kyma: Key Steps and Commands\n",
    "\n",
    "1. **Build and Push Docker Image**\n",
    "   ```bash\n",
    "   docker build -t deepan13/rag-app:latest .\n",
    "   docker push deepan13/rag-app:latest\n",
    "   ```\n",
    "\n",
    "2. **Create Kubernetes Secret for OpenAI API Key**\n",
    "   ```bash\n",
    "   kubectl create secret generic openai-credentials --from-env-file=.env\n",
    "   ```\n",
    "\n",
    "3. **Create Deployment and Service YAML**\n",
    "   File: `rag-deployment.yaml`\n",
    "   ```yaml\n",
    "   apiVersion: apps/v1\n",
    "   kind: Deployment\n",
    "   metadata:\n",
    "     name: rag-app\n",
    "   spec:\n",
    "     replicas: 1\n",
    "     selector:\n",
    "       matchLabels:\n",
    "         app: rag-app\n",
    "     template:\n",
    "       metadata:\n",
    "         labels:\n",
    "           app: rag-app\n",
    "       spec:\n",
    "         containers:\n",
    "         - name: rag-app\n",
    "           image: deepan13/rag-app:latest\n",
    "           ports:\n",
    "           - containerPort: 8080\n",
    "           envFrom:\n",
    "           - secretRef:\n",
    "               name: openai-credentials\n",
    "   ---\n",
    "   apiVersion: v1\n",
    "   kind: Service\n",
    "   metadata:\n",
    "     name: rag-service\n",
    "   spec:\n",
    "     selector:\n",
    "       app: rag-app\n",
    "     ports:\n",
    "       - protocol: TCP\n",
    "         port: 80\n",
    "         targetPort: 8080\n",
    "   ```\n",
    "\n",
    "4. **Apply Deployment and Service**\n",
    "   ```bash\n",
    "   kubectl apply -f rag-deployment.yaml\n",
    "   ```\n",
    "\n",
    "5. **Create API Rule YAML**\n",
    "   File: `rag-api-rule.yaml`\n",
    "   ```yaml\n",
    "   apiVersion: gateway.kyma-project.io/v1beta1\n",
    "   kind: APIRule\n",
    "   metadata:\n",
    "     name: rag-api-rule\n",
    "   spec:\n",
    "     gateway: kyma-gateway.kyma-system.svc.cluster.local\n",
    "     host: rag-app\n",
    "     service:\n",
    "       name: rag-service\n",
    "       port: 80\n",
    "     rules:\n",
    "       - accessStrategies:\n",
    "           - handler: noop\n",
    "         methods:\n",
    "           - GET\n",
    "           - POST\n",
    "         path: /.*\n",
    "   ```\n",
    "\n",
    "6. **Apply API Rule**\n",
    "   ```bash\n",
    "   kubectl apply -f rag-api-rule.yaml\n",
    "   ```\n",
    "\n",
    "7. **Verify Deployment**\n",
    "   ```bash\n",
    "   kubectl get pods\n",
    "   kubectl get services\n",
    "   kubectl get apirules\n",
    "   ```\n",
    "\n",
    "8. **Get Application URL**\n",
    "   ```bash\n",
    "   kubectl get apirule rag-api-rule -o jsonpath='{.status.virtualService.hosts[0]}'\n",
    "   ```\n",
    "\n",
    "9. **Access Application**\n",
    "   Open a web browser and navigate to:\n",
    "   ```\n",
    "   https://<hostname-from-previous-step>\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
