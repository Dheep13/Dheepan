{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_scaled: (8000, 4)\n",
      "Shape of X_test_scaled: (2000, 4)\n",
      "Shape of X_train_tensor: torch.Size([8000, 4])\n",
      "Shape of X_test_tensor: torch.Size([2000, 4])\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('synthetic_dataSet.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['commission'])\n",
    "y = data['commission']\n",
    "\n",
    "# Assuming X_train is already defined\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_features = ['policy_type', 'region', 'add_ons']\n",
    "encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    X[feature] = encoder.fit_transform(X[feature])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Print the shape of X_train to verify it\n",
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
    "print(\"Shape of X_train_tensor:\", X_train_tensor.shape)\n",
    "print(\"Shape of X_test_tensor:\", X_test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model with L2 regularization\n",
    "class CommissionPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden_layers, hidden_layer_size, activation_fn, l2_reg):\n",
    "        super(CommissionPredictor, self).__init__()\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation_fn = activation_fn  # Store activation function\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        # Define the layers\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_layer_size)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_layer_size, hidden_layer_size) for _ in range(num_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through input layer\n",
    "        x = self.input_layer(x)\n",
    "        x = self.activation_fn(x)  # Apply activation function to input layer output\n",
    "\n",
    "        # Forward pass through hidden layers\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            x = self.activation_fn(self.hidden_layers[_](x))  # Apply activation function to hidden layer output\n",
    "\n",
    "        # Forward pass through output layer\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def l2_loss(self):\n",
    "        l2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.sum(param ** 2)\n",
    "        return l2_loss * self.l2_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'num_hidden_layers': [1, 2, 3],           # Number of hidden layers\n",
    "    'hidden_layer_size': [64, 128, 256],      # Size of hidden layers\n",
    "    'activation_fn': [nn.ReLU(), nn.LeakyReLU()], # Activation functions\n",
    "    'lr': [0.001, 0.01],                       # Learning rate\n",
    "    'num_epochs': [50, 100],                   # Number of epochs\n",
    "    'batch_size': [32, 64],                     # Batch size\n",
    "    'l2_reg': [0.001, 0.01]                    # L2 regularization strength\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_tensor: torch.Size([8000, 4])\n",
      "Shape of y_train_tensor: torch.Size([8000])\n",
      "Shape of X_test_tensor: torch.Size([2000, 4])\n",
      "Shape of y_test_tensor: torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "# Convert hyperparameter grid into a list of dictionaries\n",
    "param_list = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Assuming X_train and y_train are defined\n",
    "# Define dataset dimensions\n",
    "input_dim = X_train.shape[1]  # Number of input features\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(\"Shape of X_train_tensor:\", X_train_tensor.shape)\n",
    "print(\"Shape of y_train_tensor:\", y_train_tensor.shape)\n",
    "print(\"Shape of X_test_tensor:\", X_test_tensor.shape)\n",
    "print(\"Shape of y_test_tensor:\", y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate model for each set of hyperparameters\n",
    "best_val_loss = np.inf\n",
    "best_model = None\n",
    "for params in param_list:\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = CommissionPredictor(input_dim, params['num_hidden_layers'], params['hidden_layer_size'], params['activation_fn'], params['l2_reg'])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        for i in range(0, len(X_train_tensor), params['batch_size']):\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = X_train_tensor[i:i+params['batch_size']]\n",
    "            batch_y = y_train_tensor[i:i+params['batch_size']]\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y) + model.l2_loss()  # Add L2 regularization to the loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "        val_loss = criterion(predictions.squeeze(), y_test_tensor)\n",
    "\n",
    "    # Update best model if validation loss is lower\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 67862.7188\n",
      "Epoch [2/50], Loss: 19761.1660\n",
      "Epoch [3/50], Loss: 10239.9111\n",
      "Epoch [4/50], Loss: 9601.0879\n",
      "Epoch [5/50], Loss: 9521.9746\n",
      "Epoch [6/50], Loss: 9498.9326\n",
      "Epoch [7/50], Loss: 9623.2959\n",
      "Epoch [8/50], Loss: 9522.7764\n",
      "Epoch [9/50], Loss: 9396.9863\n",
      "Epoch [10/50], Loss: 9406.6484\n",
      "Epoch [11/50], Loss: 9382.9844\n",
      "Epoch [12/50], Loss: 9376.6104\n",
      "Epoch [13/50], Loss: 9365.5342\n",
      "Epoch [14/50], Loss: 9359.1807\n",
      "Epoch [15/50], Loss: 9355.2959\n",
      "Epoch [16/50], Loss: 9344.7002\n",
      "Epoch [17/50], Loss: 9330.9434\n",
      "Epoch [18/50], Loss: 9321.7236\n",
      "Epoch [19/50], Loss: 9326.3623\n",
      "Epoch [20/50], Loss: 9374.5586\n",
      "Epoch [21/50], Loss: 9447.8086\n",
      "Epoch [22/50], Loss: 9491.4893\n",
      "Epoch [23/50], Loss: 9460.9824\n",
      "Epoch [24/50], Loss: 9398.8877\n",
      "Epoch [25/50], Loss: 9363.7354\n",
      "Epoch [26/50], Loss: 9355.1357\n",
      "Epoch [27/50], Loss: 9352.1201\n",
      "Epoch [28/50], Loss: 9348.6143\n",
      "Epoch [29/50], Loss: 9347.0518\n",
      "Epoch [30/50], Loss: 9348.3271\n",
      "Epoch [31/50], Loss: 9348.6162\n",
      "Epoch [32/50], Loss: 9353.7773\n",
      "Epoch [33/50], Loss: 9358.6504\n",
      "Epoch [34/50], Loss: 9364.3438\n",
      "Epoch [35/50], Loss: 9378.4160\n",
      "Epoch [36/50], Loss: 9391.6104\n",
      "Epoch [37/50], Loss: 9418.8477\n",
      "Epoch [38/50], Loss: 9452.9609\n",
      "Epoch [39/50], Loss: 9513.9570\n",
      "Epoch [40/50], Loss: 9585.3916\n",
      "Epoch [41/50], Loss: 9678.6367\n",
      "Epoch [42/50], Loss: 9786.5029\n",
      "Epoch [43/50], Loss: 9900.1123\n",
      "Epoch [44/50], Loss: 10006.5527\n",
      "Epoch [45/50], Loss: 10094.0176\n",
      "Epoch [46/50], Loss: 10174.4707\n",
      "Epoch [47/50], Loss: 10226.0078\n",
      "Epoch [48/50], Loss: 10263.8633\n",
      "Epoch [49/50], Loss: 10284.6074\n",
      "Epoch [50/50], Loss: 10296.1895\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_test and y_test are defined\n",
    "# Convert data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(X_test_tensor)\n",
    "    test_loss = criterion(predictions.squeeze(), y_test_tensor)\n",
    "    print(f'Best Validation Loss: {best_val_loss.item():.4f}')\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 10953.5869\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "model_filename = \"commission_predictor_model.pth\"\n",
    "torch.save(best_model.state_dict(), model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
