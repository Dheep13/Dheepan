{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Use Case | Definition | Performance Metrics | Key Formulas |\n",
    "|-------|----------|------------|---------------------|--------------|\n",
    "| Linear Regression | Predicting continuous values | A model that assumes a linear relationship between input features and the target variable | - Mean Squared Error (MSE)<br>- R-squared (R²)<br>- Root Mean Squared Error (RMSE) | y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε |\n",
    "| Logistic Regression | Binary classification | A model that predicts the probability of an instance belonging to a particular class | - Accuracy<br>- Precision<br>- Recall<br>- F1-score<br>- ROC AUC | P(Y=1) = 1 / (1 + e^(-z))<br>where z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ |\n",
    "| Decision Trees | Classification and regression | A tree-like model of decisions based on feature values | - Accuracy (classification)<br>- MSE (regression)<br>- Gini impurity<br>- Information gain | Gini impurity = 1 - Σ(pᵢ²)<br>where pᵢ is the probability of class i |\n",
    "| Random Forest | Classification and regression | An ensemble of decision trees | - Accuracy (classification)<br>- MSE (regression)<br>- Out-of-bag error | N/A (Ensemble of decision trees) |\n",
    "| Support Vector Machines (SVM) | Classification and regression | A model that finds the hyperplane that best separates classes in high-dimensional space | - Accuracy<br>- Margin<br>- Hinge loss | w · x - b = 0 (Linear SVM hyperplane) |\n",
    "| K-Nearest Neighbors (KNN) | Classification and regression | A model that classifies based on the majority class of K nearest neighbors | - Accuracy<br>- F1-score<br>- Distance metric (e.g., Euclidean) | Euclidean distance:<br>d(p,q) = √(Σ(pᵢ - qᵢ)²) |\n",
    "| Naive Bayes | Classification | A probabilistic model based on Bayes' theorem with independence assumptions | - Accuracy<br>- Precision<br>- Recall<br>- F1-score | P(A\\|B) = (P(B\\|A) * P(A)) / P(B) |\n",
    "| K-Means Clustering | Unsupervised clustering | A model that partitions n observations into k clusters | - Inertia<br>- Silhouette score<br>- Calinski-Harabasz index | Inertia = Σ(min(distance²) to cluster center) |\n",
    "| Principal Component Analysis (PCA) | Dimensionality reduction | A technique to reduce the dimensionality of data while preserving variance | - Explained variance ratio<br>- Cumulative explained variance | Cov(X) = (1/n) * X^T * X |\n",
    "| Neural Networks | Various (classification, regression, etc.) | A model inspired by biological neural networks, capable of learning complex patterns | - Accuracy (classification)<br>- MSE (regression)<br>- Cross-entropy loss | Activation function (e.g., ReLU):<br>f(x) = max(0, x) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Neural Network Type | Use Case | Architecture | Key Components | Activation Functions | Loss Functions | Training Algorithm | Advantages | Challenges |\n",
    "|---------------------|----------|--------------|-----------------|----------------------|-----------------|---------------------|------------|------------|\n",
    "| Feedforward Neural Network (FNN) | Classification, Regression | Input layer, hidden layer(s), output layer | Neurons, Weights, Biases | ReLU, Sigmoid, Tanh | MSE, Cross-entropy | Backpropagation | Simple, Versatile | Limited for sequential data |\n",
    "| Convolutional Neural Network (CNN) | Image Recognition, Computer Vision | Convolutional layers, Pooling layers, Fully connected layers | Filters, Feature maps | ReLU, Softmax | Cross-entropy | Gradient descent with backpropagation | Efficient for image data, Parameter sharing | Computationally intensive |\n",
    "| Recurrent Neural Network (RNN) | Sequential data, Time series, NLP | Recurrent connections | Hidden state, Input gate, Output gate | Tanh, Sigmoid | Cross-entropy, MSE | Backpropagation Through Time (BPTT) | Handles variable-length sequences | Vanishing/exploding gradients |\n",
    "| Long Short-Term Memory (LSTM) | Long-term dependencies in sequences | Memory cells, Gates (forget, input, output) | Cell state, Hidden state | Sigmoid, Tanh | Cross-entropy, MSE | Backpropagation Through Time (BPTT) | Addresses vanishing gradient problem | Complex architecture, Computationally expensive |\n",
    "| Generative Adversarial Network (GAN) | Image generation, Data augmentation | Generator and Discriminator networks | Generator, Discriminator | ReLU, Tanh, Sigmoid | Binary cross-entropy | Alternating training of Generator and Discriminator | Can generate new, realistic data | Training instability, Mode collapse |\n",
    "| Autoencoder | Dimensionality reduction, Feature learning | Encoder and Decoder networks | Encoder, Decoder, Bottleneck layer | ReLU, Sigmoid | MSE, Binary cross-entropy | Backpropagation | Unsupervised feature learning | May learn trivial solutions |\n",
    "| Transformer | NLP, Sequence-to-sequence tasks | Multi-head attention, Feed-forward layers | Self-attention, Positional encoding | ReLU, Softmax | Cross-entropy | Adam optimizer | Parallelizable, Captures long-range dependencies | High memory requirements |\n",
    "| Deep Belief Network (DBN) | Feature extraction, Dimensionality reduction | Stack of Restricted Boltzmann Machines (RBMs) | RBMs, Visible layer, Hidden layers | Sigmoid, Softmax | Contrastive divergence | Layer-wise pre-training, Fine-tuning | Unsupervised pre-training | Complex training process |\n",
    "| Radial Basis Function Network (RBFN) | Function approximation, Time series prediction | Input layer, RBF layer, Output layer | RBF neurons, Centroids | Gaussian RBF | MSE | Two-stage training (unsupervised + supervised) | Fast training, Good at interpolation | Poor extrapolation, Curse of dimensionality |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .table-container {\n",
       "        max-height: 400px;\n",
       "        overflow: auto;\n",
       "        margin-bottom: 20px;\n",
       "    }\n",
       "    table {\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    th, td {\n",
       "        border: 1px solid #ddd;\n",
       "        padding: 8px;\n",
       "        text-align: left;\n",
       "    }\n",
       "    th {\n",
       "        background-color: #f2f2f2;\n",
       "        position: sticky;\n",
       "        top: 0;\n",
       "    }\n",
       "    .code-container {\n",
       "        max-height: 300px;\n",
       "        overflow: auto;\n",
       "        border: 1px solid #ccc;\n",
       "        padding: 5px;\n",
       "    }\n",
       "    pre {\n",
       "        margin: 0;\n",
       "        white-space: pre;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<div class=\"table-container\">\n",
       "<table>\n",
       "    <tr>\n",
       "        <th>Neural Network Type</th>\n",
       "        <th>Use Case</th>\n",
       "        <th>Architecture</th>\n",
       "        <th>Key Components</th>\n",
       "        <th>Activation Functions</th>\n",
       "        <th>Loss Functions</th>\n",
       "        <th>Training Algorithm</th>\n",
       "        <th>Advantages</th>\n",
       "        <th>Challenges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Feedforward Neural Network (FNN)</td>\n",
       "        <td>Classification, Regression</td>\n",
       "        <td>Input layer, hidden layer(s), output layer</td>\n",
       "        <td>Neurons, Weights, Biases</td>\n",
       "        <td>ReLU, Sigmoid, Tanh</td>\n",
       "        <td>MSE, Cross-entropy</td>\n",
       "        <td>Backpropagation</td>\n",
       "        <td>Simple, Versatile</td>\n",
       "        <td>Limited for sequential data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Convolutional Neural Network (CNN)</td>\n",
       "        <td>Image Recognition, Computer Vision</td>\n",
       "        <td>Convolutional layers, Pooling layers, Fully connected layers</td>\n",
       "        <td>Filters, Feature maps</td>\n",
       "        <td>ReLU, Softmax</td>\n",
       "        <td>Cross-entropy</td>\n",
       "        <td>Gradient descent with backpropagation</td>\n",
       "        <td>Efficient for image data, Parameter sharing</td>\n",
       "        <td>Computationally intensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Recurrent Neural Network (RNN)</td>\n",
       "        <td>Sequential data, Time series, NLP</td>\n",
       "        <td>Recurrent connections</td>\n",
       "        <td>Hidden state, Input gate, Output gate</td>\n",
       "        <td>Tanh, Sigmoid</td>\n",
       "        <td>Cross-entropy, MSE</td>\n",
       "        <td>Backpropagation Through Time (BPTT)</td>\n",
       "        <td>Handles variable-length sequences</td>\n",
       "        <td>Vanishing/exploding gradients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Long Short-Term Memory (LSTM)</td>\n",
       "        <td>Long-term dependencies in sequences</td>\n",
       "        <td>Memory cells, Gates (forget, input, output)</td>\n",
       "        <td>Cell state, Hidden state</td>\n",
       "        <td>Sigmoid, Tanh</td>\n",
       "        <td>Cross-entropy, MSE</td>\n",
       "        <td>Backpropagation Through Time (BPTT)</td>\n",
       "        <td>Addresses vanishing gradient problem</td>\n",
       "        <td>Complex architecture, Computationally expensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Generative Adversarial Network (GAN)</td>\n",
       "        <td>Image generation, Data augmentation</td>\n",
       "        <td>Generator and Discriminator networks</td>\n",
       "        <td>Generator, Discriminator</td>\n",
       "        <td>ReLU, Tanh, Sigmoid</td>\n",
       "        <td>Binary cross-entropy</td>\n",
       "        <td>Alternating training of Generator and Discriminator</td>\n",
       "        <td>Can generate new, realistic data</td>\n",
       "        <td>Training instability, Mode collapse</td>\n",
       "    </tr>\n",
       "</table>\n",
       "</div>\n",
       "\n",
       "<div class=\"code-container\">\n",
       "<pre><code>\n",
       "# Your long code here\n",
       "# For example:\n",
       "import numpy as np\n",
       "import tensorflow as tf\n",
       "from tensorflow import keras\n",
       "\n",
       "# Define a simple feedforward neural network\n",
       "model = keras.Sequential([\n",
       "    keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
       "    keras.layers.Dense(32, activation='relu'),\n",
       "    keras.layers.Dense(16, activation='relu'),\n",
       "    keras.layers.Dense(1, activation='sigmoid')\n",
       "])\n",
       "\n",
       "# Compile the model\n",
       "model.compile(optimizer='adam',\n",
       "              loss='binary_crossentropy',\n",
       "              metrics=['accuracy'])\n",
       "\n",
       "# Generate some dummy data\n",
       "X_train = np.random.random((1000, 10))\n",
       "y_train = np.random.randint(2, size=(1000, 1))\n",
       "\n",
       "# Train the model\n",
       "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
       "\n",
       "# More code can be added here...\n",
       "</code></pre>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "html_content = \"\"\"\n",
    "<style>\n",
    "    .table-container {\n",
    "        max-height: 400px;\n",
    "        overflow: auto;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    table {\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }\n",
    "    th, td {\n",
    "        border: 1px solid #ddd;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }\n",
    "    th {\n",
    "        background-color: #f2f2f2;\n",
    "        position: sticky;\n",
    "        top: 0;\n",
    "    }\n",
    "    .code-container {\n",
    "        max-height: 300px;\n",
    "        overflow: auto;\n",
    "        border: 1px solid #ccc;\n",
    "        padding: 5px;\n",
    "    }\n",
    "    pre {\n",
    "        margin: 0;\n",
    "        white-space: pre;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"table-container\">\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Neural Network Type</th>\n",
    "        <th>Use Case</th>\n",
    "        <th>Architecture</th>\n",
    "        <th>Key Components</th>\n",
    "        <th>Activation Functions</th>\n",
    "        <th>Loss Functions</th>\n",
    "        <th>Training Algorithm</th>\n",
    "        <th>Advantages</th>\n",
    "        <th>Challenges</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Feedforward Neural Network (FNN)</td>\n",
    "        <td>Classification, Regression</td>\n",
    "        <td>Input layer, hidden layer(s), output layer</td>\n",
    "        <td>Neurons, Weights, Biases</td>\n",
    "        <td>ReLU, Sigmoid, Tanh</td>\n",
    "        <td>MSE, Cross-entropy</td>\n",
    "        <td>Backpropagation</td>\n",
    "        <td>Simple, Versatile</td>\n",
    "        <td>Limited for sequential data</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Convolutional Neural Network (CNN)</td>\n",
    "        <td>Image Recognition, Computer Vision</td>\n",
    "        <td>Convolutional layers, Pooling layers, Fully connected layers</td>\n",
    "        <td>Filters, Feature maps</td>\n",
    "        <td>ReLU, Softmax</td>\n",
    "        <td>Cross-entropy</td>\n",
    "        <td>Gradient descent with backpropagation</td>\n",
    "        <td>Efficient for image data, Parameter sharing</td>\n",
    "        <td>Computationally intensive</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Recurrent Neural Network (RNN)</td>\n",
    "        <td>Sequential data, Time series, NLP</td>\n",
    "        <td>Recurrent connections</td>\n",
    "        <td>Hidden state, Input gate, Output gate</td>\n",
    "        <td>Tanh, Sigmoid</td>\n",
    "        <td>Cross-entropy, MSE</td>\n",
    "        <td>Backpropagation Through Time (BPTT)</td>\n",
    "        <td>Handles variable-length sequences</td>\n",
    "        <td>Vanishing/exploding gradients</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Long Short-Term Memory (LSTM)</td>\n",
    "        <td>Long-term dependencies in sequences</td>\n",
    "        <td>Memory cells, Gates (forget, input, output)</td>\n",
    "        <td>Cell state, Hidden state</td>\n",
    "        <td>Sigmoid, Tanh</td>\n",
    "        <td>Cross-entropy, MSE</td>\n",
    "        <td>Backpropagation Through Time (BPTT)</td>\n",
    "        <td>Addresses vanishing gradient problem</td>\n",
    "        <td>Complex architecture, Computationally expensive</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Generative Adversarial Network (GAN)</td>\n",
    "        <td>Image generation, Data augmentation</td>\n",
    "        <td>Generator and Discriminator networks</td>\n",
    "        <td>Generator, Discriminator</td>\n",
    "        <td>ReLU, Tanh, Sigmoid</td>\n",
    "        <td>Binary cross-entropy</td>\n",
    "        <td>Alternating training of Generator and Discriminator</td>\n",
    "        <td>Can generate new, realistic data</td>\n",
    "        <td>Training instability, Mode collapse</td>\n",
    "    </tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "<div class=\"code-container\">\n",
    "<pre><code>\n",
    "# Your long code here\n",
    "# For example:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate some dummy data\n",
    "X_train = np.random.random((1000, 10))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# More code can be added here...\n",
    "</code></pre>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "HTML(html_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
