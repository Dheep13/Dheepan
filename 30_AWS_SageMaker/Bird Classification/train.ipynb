{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision tqdm\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from google.colab import drive\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print GPU info\n",
    "!nvidia-smi\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define a directory in your Google Drive to save checkpoints\n",
    "checkpoint_dir = '/content/drive/MyDrive/bird_classification_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageFolder(root='/content/Upload/Upload/train', transform=transform)\n",
    "val_dataset = ImageFolder(root='/content/Upload/Upload/valid', transform=transform)\n",
    "test_dataset = ImageFolder(root='/content/Upload/Upload/test', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128  # Increased for A100\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "model = create_model(num_classes).to(device)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    filepath = os.path.join(checkpoint_dir, filename)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    filepath = os.path.join(checkpoint_dir, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Checkpoint loaded: {filepath}\")\n",
    "        return epoch, loss\n",
    "    return 0, 0  # If no checkpoint found, start from beginning\n",
    "\n",
    "# Load the latest checkpoint if it exists\n",
    "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files)\n",
    "    start_epoch, start_loss = load_checkpoint(model, optimizer, latest_checkpoint)\n",
    "else:\n",
    "    start_epoch, start_loss = 0, 0\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler()\n",
    "\n",
    "num_epochs = 10\n",
    "total_batches = len(train_loader)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'loss': running_loss / (i+1)})\n",
    "            \n",
    "            if (i + 1) % (total_batches // 10) == 0:\n",
    "                save_checkpoint(model, optimizer, epoch, running_loss / (i+1), f\"checkpoint_epoch_{epoch+1}_{int((i+1)/total_batches*100)}.pth\")\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {running_loss / total_batches:.4f}')\n",
    "    save_checkpoint(model, optimizer, epoch + 1, running_loss / total_batches, f\"checkpoint_epoch_{epoch+1}_complete.pth\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_accuracy = evaluate(model, val_loader)\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = evaluate(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/bird_classification_final_model.pth')\n",
    "print(\"Final model saved to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
